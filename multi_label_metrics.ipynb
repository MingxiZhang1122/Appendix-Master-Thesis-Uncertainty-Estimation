{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "import keras_tuner as kt\n",
    "\n",
    "#macro post evaluation Recall, Precision, F1 & Acc each Label\n",
    "def recall_macro(y_true,y_pred):\n",
    "    true_positives = np.sum(y_true * y_pred, axis=0)\n",
    "    possible_positives = np.sum(y_true, axis=0)\n",
    "    recall_each_label = true_positives / possible_positives\n",
    "    recall_macro = np.mean(recall_each_label)\n",
    "    return recall_each_label, recall_macro\n",
    "\n",
    "def precision_macro(y_true,y_pred):\n",
    "    true_positives = np.sum(y_true * y_pred, axis=0)\n",
    "    predicted_positives = possible_positives = np.sum(y_pred, axis=0)\n",
    "    precision_each_label = true_positives / predicted_positives\n",
    "    precision_macro = np.mean(precision_each_label)\n",
    "    return precision_each_label, precision_macro\n",
    "def f1_macro(y_true,y_pred):\n",
    "    p = precision_macro(y_true,y_pred)[0]\n",
    "    r = recall_macro(y_true,y_pred)[0]\n",
    "    f1_each_label = 2*((p*r)/(p+r))\n",
    "    f1_macro = np.mean(f1_each_label)    \n",
    "    return f1_each_label,f1_macro\n",
    "def accuracy_each_label(y_true,y_pred):\n",
    "    acc_prediction = np.count_nonzero(y_true - y_pred==0,axis=0)\n",
    "    acc = acc_prediction / np.shape(y_true)[0]\n",
    "    return acc\n",
    "    \n",
    "#micro integrated in Model Recall, Precision, F1\n",
    "def recall_micro(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_micro = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_micro\n",
    "def precision_micro(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_micro = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_micro\n",
    "def f1_micro(y_true, y_pred):\n",
    "    p = precision_micro(y_true, y_pred)\n",
    "    r = recall_micro(y_true, y_pred)\n",
    "    f1_micro= 2*((p*r)/(p+r+K.epsilon()))\n",
    "    return f1_micro\n",
    "\n",
    "#hamming integrated in model\n",
    "def hamming_loss(y_true, y_pred):\n",
    "    temp = K.abs(y_true-y_pred)\n",
    "    return K.mean(K.cast(K.greater(temp,0.5),dtype=float))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
